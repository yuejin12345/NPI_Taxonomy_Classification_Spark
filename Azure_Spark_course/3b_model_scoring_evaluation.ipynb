{"cells":[{"cell_type":"markdown","source":["# Step 3B: Model Scoring evaluation\n\nUsing the results data set constructed in the `3b_model_scoring` Jupyter notebook, this notebook loads the data scores the observations. \n\n**Note:** This notebook will take about 1 minutes to execute all cells, depending on the compute configuration you have setup."],"metadata":{}},{"cell_type":"code","source":["# import the libraries\n\n# For some data handling\nimport numpy as np\nfrom pyspark.ml import PipelineModel\n# for creating pipelines and model\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n\n# The scoring uses the same feature engineering script used to train the model\nresults_table = 'results_output'"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["dbutils.widgets.removeAll()\ndbutils.widgets.text(\"results_data\", results_table)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# make predictions. The Pipeline does all the same operations on the test data\nsqlContext.refreshTable(dbutils.widgets.get(\"results_data\")) \npredictions =  spark.table(dbutils.widgets.get(\"results_data\"))\n\n# Create the confusion matrix for the multiclass prediction results\n# This result assumes a decision boundary of p = 0.5\nconf_table = predictions.stat.crosstab('indexedLabel', 'prediction')\nconfuse = conf_table.toPandas()\nconfuse.head()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["The confusion matrix lists each true component failure in rows and the predicted value in columns. Labels numbered 0.0 corresponds to no component failures. Labels numbered 1.0 through 4.0 correspond to failures in one of the four components in the machine. As an example, the third number in the top row indicates how many days we predicted component 2 would fail, when no components actually did fail. The second number in the second row, indicates how many days we correctly predicted a component 1 failure within the next 7 days.\n\nWe read the confusion matrix numbers along the diagonal as correctly classifying the component failures. Numbers above the diagonal indicate the model incorrectly predicting a failure when non occured, and those below indicate incorrectly predicting a non-failure for the row indicated component failure.\n\nWhen evaluating classification models, it is convenient to reduce the results in the confusion matrix into a single performance statistic. However, depending on the problem space, it is impossible to always use the same statistic in this evaluation. Below, we calculate four such statistics.\n\n- **Accuracy**: reports how often we correctly predicted the labeled data. Unfortunatly, when there is a class imbalance (a large number of one of the labels relative to others), this measure is biased towards the largest class. In this case non-failure days.\n\nBecause of the class imbalance inherint in predictive maintenance problems, it is better to look at the remaining statistics instead. Here positive predictions indicate a failure.\n\n- **Precision**: Precision is a measure of how well the model classifies the truely positive samples. Precision depends on falsely classifying negative days as positive.\n\n- **Recall**: Recall is a measure of how well the model can find the positive samples. Recall depends on falsely classifying positive days as negative.\n\n- **F1**: F1 considers both the precision and the recall. F1 score is the harmonic average of precision and recall. An F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n\nThese metrics make the most sense for binary classifiers, though they are still useful for comparision in our multiclass setting. Below we calculate these evaluation statistics for the selected classifier."],"metadata":{}},{"cell_type":"code","source":["# select (prediction, true label) and compute test error\n# select (prediction, true label) and compute test error\n# True positives - diagonal failure terms \ntp = confuse['1.0'][1]+confuse['2.0'][2]+confuse['3.0'][3]+confuse['4.0'][4]\n\n# False positves - All failure terms - True positives\nfp = np.sum(np.sum(confuse[['1.0', '2.0','3.0','4.0']])) - tp\n\n# True negatives \ntn = confuse['0.0'][0]\n\n# False negatives total of non-failure column - TN\nfn = np.sum(np.sum(confuse[['0.0']])) - tn\n\n# Accuracy is diagonal/total \nacc_n = tn + tp\nacc_d = np.sum(np.sum(confuse[['0.0','1.0', '2.0','3.0','4.0']]))\nacc = acc_n/acc_d\n\n# Calculate precision and recall.\nprec = tp/(tp+fp)\nrec = tp/(tp+fn)\n\n# Print the evaluation metrics to the notebook\nprint(\"Accuracy = %g\" % acc)\nprint(\"Precision = %g\" % prec)\nprint(\"Recall = %g\" % rec )\nprint(\"F1 = %g\" % (2.0 * prec * rec/(prec + rec)))\nprint(\"\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Remember that this is a simulated data set. We would expect a model built on real world data to behave very differently. The accuracy may still be close to one, but the precision and recall numbers would be much lower."],"metadata":{}},{"cell_type":"code","source":["predictions.toPandas().head(20)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["print(predictions.summary())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["predictions.explain()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Conclusion\n\nThis concludes this scenario. You can modify these notebooks to customize your own use case solution."],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"PredictiveMaintenance dlvmjme","language":"python","name":"predictivemaintenance_dlvmjme"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"4_model_scoring","notebookId":1465148638320961},"nbformat":4,"nbformat_minor":0}
