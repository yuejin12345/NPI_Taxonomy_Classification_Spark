{
    "cells": [{
        "cell_type": "markdown",
        "source": ["# Step 3B: Model Scoring\n\nUsing a scoring data set constructed in the `2a_feature_engineering` notebook, and a model constructed in the `2b_model_building` notebook (this is run through the `2_Training_Pipeline` notebook, this notebook loads the data and predicts the probability of component failure with the provided model. \n\nThis notebook can be run though the `3_Scoring_Pipeline` notebook, which creates a temporary scoring data set before scoring the data with this notebook.\n\nWe provide the `3b_model_scoring_evalation` notebook to examine the output of the scoring process.\n\n**IMPORTANT NOTE** This notebook depends on there being a `scoring_data` set in the Databricks Data store. You can score any dataset constructed with the `2a_feature_engineering` notebook, but you must specify that data set in the `scoring_data` parameter above, or this notebook will fail. \n\n**Note:** This notebook should take less than a minute to execute all cells, depending on the compute configuration you have setup."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["# import the libraries\nfrom pyspark.ml import PipelineModel\n# for creating pipelines and model\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n\n# The scoring uses the same feature engineering script used to train the model\nscoring_table = 'testing_data'\nresults_table = 'results_output'\n\nmodel = 'RandomForest' # Use 'DecisionTree' or 'RandomForest'"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 2
    }, {
        "cell_type": "markdown",
        "source": ["Databricks parameters to customize the runs."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["dbutils.widgets.removeAll()\ndbutils.widgets.text(\"scoring_data\", scoring_table)\ndbutils.widgets.text(\"results_data\", results_table)\n\ndbutils.widgets.text(\"model\", model)"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 4
    }, {
        "cell_type": "markdown",
        "source": ["We need to run the feature engineering on the data we're interested in scoring (`2a_feature_engineering`). Spark MLlib models require a vectorized data frame. We transform the dataset here for model consumption. In a general scoring operation, we do not know the labels so we only need to construct the features."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["sqlContext.refreshTable(dbutils.widgets.get(\"scoring_data\")) \nscore_data = spark.table(dbutils.widgets.get(\"scoring_data\"))\n\n# We'll use the known label, and key variables.\nlabel_var = ['label_e']\nkey_cols =['machineID','dt_truncated']\n\n# Then get the remaing feature names from the data\ninput_features = score_data.columns\n\n# We'll use the known label, key variables and \n# a few extra columns we won't need.\nremove_names = label_var + key_cols + ['failure','model_encoded','model' ]\n\n# Remove the extra names if that are in the input_features list\ninput_features = [x for x in input_features if x not in set(remove_names)]\n\ninput_features\n# assemble features\nva = VectorAssembler(inputCols=(input_features), outputCol='features')\n\n# assemble features\nscore_data = va.transform(score_data).select('machineID','dt_truncated','label_e','features')\n\n# set maxCategories so features with > 10 distinct values are treated as continuous.\nfeatureIndexer = VectorIndexer(inputCol=\"features\", \n                               outputCol=\"indexedFeatures\", \n                               maxCategories=10).fit(score_data)"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 6
    }, {
        "cell_type": "markdown",
        "source": ["To evaluate this model, we predict the component failures over the test data set. Since the test set has been created from data the model has not been seen before, it simulates future data. The evaluation then can be generalize to how the model could perform when operationalized and used to score the data in real time."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["# Load the model from local storage\nmodel_pipeline = PipelineModel.load(\"dbfs:/storage/models/\" + dbutils.widgets.get(\"model\") + \".pqt\")\n\n# score the data. The Pipeline does all the same operations on this dataset\npredictions = model_pipeline.transform(score_data)\n\n#write results to data store for persistance.\npredictions.write.mode('overwrite').saveAsTable(dbutils.widgets.get(\"results_data\"))"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 8
    }, {
        "cell_type": "markdown",
        "source": ["# Conclusion\n\nWe have provided an additional notebook (`3a_model_scoring_evaluation`) to examine how the process works."],
        "metadata": {}
    }],
    "metadata": {
        "kernelspec": {
            "display_name": "PredictiveMaintenance dlvmjme",
            "language": "python",
            "name": "predictivemaintenance_dlvmjme"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "version": "3.5.2",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "name": "4_model_scoring",
        "notebookId": 1086115452232417
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
